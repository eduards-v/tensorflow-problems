{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data set learning using TensorFlow\n",
    "\n",
    "Guide link: https://www.tensorflow.org/get_started/mnist/beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Implementing model\n",
    "\n",
    "# Input value for TF running session. Would be a flatten image from MNIST dataset. 784 = 28x28 pixel image\n",
    "# placeholder - a value that we'll input when we ask TensorFlow to run a computation.\n",
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "\n",
    "# Weight - zeros paramas: diamensional image vectors | 10-dimensional vectors represent number classes in range of 0-9\n",
    "W = tf.Variable(tf.zeros([784, 10])) \n",
    "\n",
    "b = tf.Variable(tf.zeros([10])) # Bias added to a to output\n",
    "\n",
    "# Create regression model using values above. \n",
    "y = tf.matmul(x, W) + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This set will be storing predictions\n",
    "# Each row is a one-hot 10-dimensional vector indicating which digit class \n",
    "# (zero through nine) the corresponding MNIST image belongs to.\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "# Cross entropy - the average length of communicating an event from one distribution \n",
    "# with the optimal code for another distribution. Ref.: http://colah.github.io/posts/2015-09-Visual-Information/\n",
    "\n",
    "# In this case, it will help to predict how close TF prediction of a number is to the actual correct answer.\n",
    "# Loss function indicates how bad the model's prediction was on a single example; \n",
    "# we try to minimize that while training across all the examples.\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "# Pick optimization algorithm.\n",
    "# GradientDescentOptimizer defines a rate at which TF is going to descend the cross entropy. \n",
    "# We set it to 0.5, but should be adjusted.\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Create TF study session. Session is connecting to TF backend written in C++ to do heavy computation.\n",
    "# InteractiveSession is a convenient way to build cumputation graphs. If you not using InteractiveSession,\n",
    "# you will have to build \n",
    "sess = tf.InteractiveSession()\n",
    "# Initialize variables for TF\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# We are going to run training step 1000 times. Again, it should be adjusted for optimal learning time.\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(50) # Picks 100 random data points from set. \n",
    "                                                   # More data TF algorithms use to train, better accuracy is produces.\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # Run training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9155\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model\n",
    "# Handpick correct predictions from y_ set\n",
    "# tf.argmax(y,1) is the label our model thinks is most likely for each input, \n",
    "# while tf.argmax(y_,1) is the true label.\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "# Evaluate accuracy on the test data\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Print correct prediction rate \n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving accuracy with neural networking\n",
    "\n",
    "Tutorial ref.: https://www.tensorflow.org/get_started/mnist/pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to generate weight and bias nodes used for Neural Network model.\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1) # slightly increase starting weight to 0.1 to prevent 0 gradients.\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  # it is also good practice to initialize neurons with a slightly positive initial bias to avoid \"dead neurons\". \n",
    "  initial = tf.constant(0.1, shape=shape) \n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution and Pooling functions\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First Layer - consist of convolution, followed by max pooling.\n",
    "\n",
    "# The first two dimensions are the patch size, the next is the number of input channels,\n",
    "# and the last is the number of output channels.\n",
    "W_conv1 = weight_variable([5, 5, 1, 32]) \n",
    "\n",
    "# a bias vector with a component for each output channel\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# reshape x to 4d tensor where second and third dimensions is image width and height, and last is the number of color channels\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool. \n",
    "# The max_pool_2x2 method will reduce the image size to 14x14.\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second Convolutional Layer\n",
    "# In order to build a deep network, we stack several layers of this type. \n",
    "# The second layer will have 64 features for each 5x5 patch.\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Densely Connected Layer\n",
    "# Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons \n",
    "# to allow processing on the entire image. We reshape the tensor from the pooling layer into a batch of vectors, \n",
    "# multiply by a weight matrix, add a bias, and apply a ReLU.\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout - to reduce overfitting\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Readout Layer\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.16\n",
      "step 100, training accuracy 0.82\n",
      "step 200, training accuracy 0.94\n",
      "step 300, training accuracy 0.88\n",
      "step 400, training accuracy 0.98\n",
      "step 500, training accuracy 0.92\n",
      "step 600, training accuracy 0.94\n",
      "step 700, training accuracy 0.94\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 0.98\n",
      "step 1100, training accuracy 0.96\n",
      "step 1200, training accuracy 0.92\n",
      "step 1300, training accuracy 0.96\n",
      "step 1400, training accuracy 0.98\n",
      "step 1500, training accuracy 0.92\n",
      "step 1600, training accuracy 1\n",
      "step 1700, training accuracy 0.94\n",
      "step 1800, training accuracy 0.94\n",
      "step 1900, training accuracy 1\n",
      "step 2000, training accuracy 0.98\n",
      "step 2100, training accuracy 0.96\n",
      "step 2200, training accuracy 1\n",
      "step 2300, training accuracy 0.98\n",
      "step 2400, training accuracy 1\n",
      "step 2500, training accuracy 1\n",
      "step 2600, training accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(5000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "  print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
